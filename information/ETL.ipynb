{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjomsZozKJpZ"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install \"pymongo[srv]\"\n",
    "!pip install dnspython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U25R5d0aKY0K"
   },
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgvBQzuDxuQB"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "# pip3 install pymongo[srv]\n",
    "\n",
    "\n",
    "def data_extract():\n",
    "    host = \"2kbot.dp2to.mongodb.net/users-linkedin-scraped?retryWrites=true&ssl=true&ssl_cert_reqs=CERT_NONE\"\n",
    "\n",
    "    user = 'users-linkedin-scraped'\n",
    "    password = 'IsVSEzCPGhUJFQJe'\n",
    "    uri = \"mongodb+srv://%s:%s@%s\" % (quote_plus(user), quote_plus(password), host)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client.get_database('users-linkedin-scraped')\n",
    "    #print(db.list_collection_names())\n",
    "    collection_employees = db.get_collection(\"users\").find()\n",
    "    list_cur = list(collection_employees)\n",
    "    df_raw = pd.DataFrame(list_cur)\n",
    "    # print(df_raw.head())\n",
    "    #print(df_raw.columns)\n",
    "    return df_raw\n",
    "\n",
    "\n",
    "def companies_data_extract():\n",
    "    host = \"2kbot.dp2to.mongodb.net/users-linkedin-scraped?retryWrites=true&ssl=true&ssl_cert_reqs=CERT_NONE\"\n",
    "\n",
    "    user = 'users-linkedin-scraped'\n",
    "    password = 'IsVSEzCPGhUJFQJe'\n",
    "    uri = \"mongodb+srv://%s:%s@%s\" % (quote_plus(user), quote_plus(password), host)\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client.get_database('users-linkedin-scraped')\n",
    "\n",
    "    collection_employees = db.get_collection(\"companies\").find()\n",
    "    list_cur = list(collection_employees)\n",
    "    df_raw = pd.DataFrame(list_cur)\n",
    "    return df_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCD14Vaxx0bC"
   },
   "source": [
    "# Data transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "837PCQ4WJs8O"
   },
   "source": [
    "## Companies transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZxDjDG6Jndg"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def cat_company_size(siz):\n",
    "    try:\n",
    "        resp = None\n",
    "        if siz:\n",
    "            size_dict = {0: [1, None],\n",
    "                         1: [1, 10],\n",
    "                         2: [11, 50],\n",
    "                         3: [51, 200],\n",
    "                         4: [201, 500],\n",
    "                         5: [501, 1000],\n",
    "                         6: [1001, 5000],\n",
    "                         7: [5001, 10000],\n",
    "                         8: [10001, None]}\n",
    "            for key, value in size_dict.items():\n",
    "                if siz == value:\n",
    "                    resp = key\n",
    "        else:\n",
    "            resp = 0\n",
    "        if not resp:\n",
    "            resp = 0\n",
    "        return resp\n",
    "    except:\n",
    "        logging.exception(\"Error in companies: \")\n",
    "\n",
    "\n",
    "def homogen_companies(company):\n",
    "    company_cln = company.lower().replace(\"é\", \"e\").replace(\"è\", \"e\").replace(\"à\", \"a\").replace(\",\", \"\") \\\n",
    "        .replace(\"ó\", \"o\").replace(\"á\", \"a\").replace(\".\", \"\").replace(\"-\", \"\").replace(\"(\", \"\").replace(\")\", \"\") \\\n",
    "        .replace(\"'\", \"\").replace(\" \", \"\").replace(\"espa%c3%b1a\", \"\")\n",
    "    return company_cln\n",
    "\n",
    "\n",
    "def check_freelancer(job, size_cat):\n",
    "    if job and any(x in job for x in [\"freelance\", \"self employed\", \"self-employed\", \"autonomo\", \"freelancing\"]):\n",
    "        return 1\n",
    "    else:\n",
    "        return size_cat\n",
    "\n",
    "\n",
    "def find_company(experiencia, i):\n",
    "    try:\n",
    "        comp = homogen_companies(experiencia[i]['company'])\n",
    "    except:\n",
    "        comp = None\n",
    "    return comp\n",
    "\n",
    "\n",
    "def search_comp_name_in_comp(lista_comp, comp):\n",
    "    if comp:\n",
    "        for c in lista_comp:\n",
    "            if comp in c:\n",
    "                comp = c\n",
    "                break\n",
    "    return comp\n",
    "\n",
    "\n",
    "def search_comp_name_in_user(companies, comp):\n",
    "    if comp:\n",
    "        for c in companies:\n",
    "            if c in comp and len(c) > 3:\n",
    "                comp = c\n",
    "                break\n",
    "    return comp\n",
    "\n",
    "\n",
    "def company_data():\n",
    "    df_raw = companies_data_extract()\n",
    "    df_cln = df_raw[['industry', 'company_size', 'locations', 'name', 'universal_name_id', 'similar_companies']]\n",
    "\n",
    "    industries = df_cln.industry.unique().tolist()\n",
    "    i = 0\n",
    "    dict_indus = {}\n",
    "    dict_indus_type = {}\n",
    "    l = []\n",
    "    l.extend(range(0, len(industries)+1))\n",
    "    a = to_categorical(l, num_classes=len(industries)+1)\n",
    "    for industry in industries:\n",
    "        dict_indus[industry] = i\n",
    "        dict_indus_type[i] = a[i]\n",
    "        i += 1\n",
    "\n",
    "    df_cln['industries_cat'] = df_cln.apply(lambda row: dict_indus_type[dict_indus[row.industry]], axis=1)\n",
    "\n",
    "    df_cln['size_cat'] = df_cln.apply(lambda row: cat_company_size(row.company_size), axis=1)\n",
    "    df_cln.loc[df_cln.universal_name_id == '', 'universal_name_id'] = df_cln['name']\n",
    "    df_cln.loc[df_cln.universal_name_id == 'ing', 'universal_name_id'] = \"ing direct\"\n",
    "    df_cln.universal_name_id.fillna(df_cln.name, inplace=True)\n",
    "    df_cln['universal_name_id'] = df_cln.apply(lambda row: homogen_companies(row.universal_name_id), axis=1)\n",
    "\n",
    "    return df_cln\n",
    "\n",
    "\n",
    "def merge_comp(df1, df2, i):\n",
    "    dfm = df1.merge(df2, on=f'job_{i}_company', how='left', suffixes=('', f'_{i}'))\n",
    "    return dfm\n",
    "\n",
    "\n",
    "def company_enrichment(df, df_comp, i, dict_size):\n",
    "    companies = df_comp.universal_name_id.unique().tolist()\n",
    "    df_comp = df_comp.rename(columns={'universal_name_id': f'job_{i}_company', 'size_cat': f'size_cat{i}'})\n",
    "    df[f'job_{i}_company'] = df.apply(lambda row: find_company(row.experiences, i - 1), axis=1)\n",
    "    # primer intento: a lo burro\n",
    "    df_raw_cols = list(df)\n",
    "    df_enrich = merge_comp(df, df_comp, i).drop_duplicates(subset=['_id'])\n",
    "    df_enrich[f'size_cat{i}'] = df_enrich.apply(lambda row: check_freelancer(row[f'job_{i}_company'], row[f'size_cat{i}']),\n",
    "                                            axis=1)\n",
    "    df_temp = df_enrich.loc[df_enrich[f'size_cat{i}'].isnull()]\n",
    "    df_temp = df_temp[df_raw_cols]\n",
    "    df_enrich = df_enrich.loc[~df_enrich[f'size_cat{i}'].isnull()]\n",
    "    df_temp[f'job_{i}_company'] = df_temp.apply(\n",
    "        lambda row: search_comp_name_in_comp(companies, row[f'job_{i}_company']), axis=1)\n",
    "    df_temp = merge_comp(df_temp, df_comp, i)\n",
    "    df_enrich = df_enrich.append(df_temp).drop_duplicates(subset=['_id'])\n",
    "    df_temp = df_enrich.loc[df_enrich[f'size_cat{i}'].isnull()]\n",
    "    df_temp = df_temp[df_raw_cols]\n",
    "    df_enrich = df_enrich.loc[~df_enrich[f'size_cat{i}'].isnull()]\n",
    "    df_temp[f'job_{i}_company'] = df_temp.apply(\n",
    "        lambda row: search_comp_name_in_user(companies, row[f'job_{i}_company']), axis=1)\n",
    "    df_temp = merge_comp(df_temp, df_comp, i)\n",
    "    df_enrich = df_enrich.append(df_temp).drop_duplicates(subset=['_id'])\n",
    "    df_enrich.loc[df_enrich[f'size_cat{i}'].isnull(), f'size_cat{i}'] = 9\n",
    "    df_enrich[f'size_cat{i}_class'] = df_enrich.apply(lambda row: dict_size[int(row[f'size_cat{i}'])], axis=1)\n",
    "    return df_enrich\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvpwLvKbJy4P"
   },
   "source": [
    "## Jobs recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lFhOsjlNJ_7",
    "outputId": "ce7642c6-bb42-40ce-fae9-618b3f0d38b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "global words_set\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('stopwords')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "def get_training_data():\n",
    "    global words_set\n",
    "    words_set = []\n",
    "    with open('drive/MyDrive/databases/dict_num.json') as json_file:\n",
    "        dict_num = json.load(json_file)\n",
    "    init_df = pd.read_csv('drive/MyDrive/databases/init_df.csv')\n",
    "    with open('drive/MyDrive/databases/job_cat.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    next_df = pd.DataFrame({'Titulo': data.keys(), 'OHE_Label': data.values()})\n",
    "\n",
    "    one_hot_encoding = []\n",
    "    final_shape = max(init_df['OHE_Label'].to_list())\n",
    "    for i in init_df['OHE_Label']:\n",
    "        vector = [0] * final_shape\n",
    "        vector[i - 1] = 1\n",
    "        one_hot_encoding.append(vector)\n",
    "    init_df['OHE_Label'] = one_hot_encoding\n",
    "\n",
    "    one_hot_encoding = []\n",
    "    final_shape = max(next_df['OHE_Label'].to_list())\n",
    "    for i in next_df['OHE_Label']:\n",
    "        vector = [0] * final_shape\n",
    "        vector[i - 1] = 1\n",
    "        one_hot_encoding.append(vector)\n",
    "    next_df['OHE_Label'] = one_hot_encoding\n",
    "    next_df['BoW'] = [None] * next_df.shape[0]\n",
    "\n",
    "    tercer_df = pd.read_csv('drive/MyDrive/databases/tercer_dataset.csv', sep=';')\n",
    "\n",
    "    tercer_df['Codigo'] = [math.trunc(x / 100) for x in tercer_df['Codigo']]\n",
    "    tercer_df = tercer_df[tercer_df['Codigo'] > 0]\n",
    "    tercer_df['OHE_Label'] = [dict_num[str(x)] for x in tercer_df['Codigo']]\n",
    "\n",
    "    one_hot_encoding = []\n",
    "    final_shape = max(tercer_df['OHE_Label'].to_list())\n",
    "    for i in tercer_df['OHE_Label']:\n",
    "        vector = [0] * final_shape\n",
    "        vector[i - 1] = 1\n",
    "        one_hot_encoding.append(vector)\n",
    "    tercer_df['OHE_Label'] = one_hot_encoding\n",
    "    tercer_df['BoW'] = [None] * tercer_df.shape[0]\n",
    "    tercer_df = tercer_df[['Titulo', 'BoW', 'OHE_Label']]\n",
    "\n",
    "    df = init_df.append(next_df, ignore_index=True)\n",
    "    df = df.append(tercer_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(palabras):\n",
    "    spanish_stemmer = SnowballStemmer('spanish')\n",
    "    global words_set\n",
    "\n",
    "    sotpwords_set = list(set(stopwords.words('spanish')))\n",
    "    sotpwords_set.append('(')\n",
    "    sotpwords_set.append(')')\n",
    "    sotpwords_set.append('[')\n",
    "    sotpwords_set.append(']')\n",
    "    sotpwords_set.append(';')\n",
    "    sotpwords_set.append(',')\n",
    "    sotpwords_set.append(':')\n",
    "    final_text = []\n",
    "\n",
    "    for word in palabras:\n",
    "        if word not in sotpwords_set:\n",
    "            final_word = word.lower()\n",
    "            index_dot = final_word.find('.')\n",
    "            if index_dot != -1:\n",
    "                final_word = final_word[:index_dot]\n",
    "            index_dot = final_word.find(',')\n",
    "            if index_dot != -1:\n",
    "                final_word = final_word[:index_dot]\n",
    "            index_dot = final_word.find('-')\n",
    "            if index_dot != -1:\n",
    "                final_word = final_word[:index_dot]\n",
    "            final_word = spanish_stemmer.stem(final_word)\n",
    "            words_set.append(final_word)\n",
    "            words_set = list(set(words_set))\n",
    "            final_text.append(final_word)\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "def create_model(df_train):\n",
    "\n",
    "    toktok = ToktokTokenizer()\n",
    "    textos_tokenizados = []\n",
    "\n",
    "    for text in df_train['Titulo']:\n",
    "        tokens = toktok.tokenize(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        textos_tokenizados.append(tokens)\n",
    "    global words_set\n",
    "    dictionary = {}\n",
    "    count = 1\n",
    "    for i in words_set:\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "\n",
    "    bow_corpus = []\n",
    "    n = len(words_set)\n",
    "    for i in textos_tokenizados:\n",
    "        bow_text = [0] * n\n",
    "        for j in i:\n",
    "            bow_text[dictionary[j] - 1] = 1\n",
    "        bow_corpus.append(bow_text)\n",
    "    df_train['BoW'] = bow_corpus\n",
    "\n",
    "    x = df_train['BoW'].to_list()\n",
    "    y = df_train['OHE_Label'].to_list()\n",
    "\n",
    "    init_shape = len(x[0])\n",
    "    final_shape = len(y[0])\n",
    "    print(f\"init: {init_shape}\")\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(init_shape, input_dim=init_shape, activation='relu'))\n",
    "    model.add(layers.Dense(1500, activation='relu'))\n",
    "    model.add(layers.Dense(1000, activation='relu'))\n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(final_shape, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x, y, epochs=20, batch_size=64, verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    toktok = ToktokTokenizer()\n",
    "    global words_set\n",
    "    tokens = toktok.tokenize(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    n = len(words_set)\n",
    "    bow_text = [0] * n\n",
    "    dictionary = {}\n",
    "    count = 1\n",
    "    for i in words_set:\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "\n",
    "    for j in tokens:\n",
    "        try:\n",
    "            bow_text[dictionary[j] - 1] = 1\n",
    "        except:\n",
    "            pass\n",
    "            # print('La palabra', j, 'no esta en el diccionario.')\n",
    "    return bow_text\n",
    "\n",
    "\n",
    "def pred_jobs(model, df_raw, ind):\n",
    "    bow = df_raw[f'BoW_{ind}'].to_list()\n",
    "    # for i in bow:\n",
    "    #    print(len(i))\n",
    "    pred = model.predict(bow)\n",
    "    # print(pred)\n",
    "\n",
    "    pred_num = []\n",
    "    for ind in pred:\n",
    "        max_index = np.argmax(ind) + 1\n",
    "        pred_num.append(max_index)\n",
    "    df_raw[f'job_{ind}_category'] = pred_num\n",
    "    return df_raw[f'job_{ind}_category']\n",
    "\n",
    "\n",
    "def jobs_prediction(df):\n",
    "    print(\"\\n Starting jobs prediction...\")\n",
    "    with open('drive/MyDrive/databases/dict_change.json') as file:\n",
    "        dict_change = json.load(file)\n",
    "    df_train = get_training_data()\n",
    "    for i in range(1, 7):\n",
    "        df_temp = df[[f'job_{i}_category', f'job_{i}']]\n",
    "        df_temp[f'BoW_{i}'] = df_temp[f'job_{i}'].apply(preprocess_text)\n",
    "    del df_temp\n",
    "\n",
    "    try:\n",
    "        model = keras.models.load_model(\"drive/MyDrive/databases/jobs_prediction\")\n",
    "        print(\"model loaded\")\n",
    "    except:\n",
    "      try:\n",
    "        model = keras.models.load_model(\"drive/MyDrive/databases/jobs_prediction\")\n",
    "        print(\"model loaded\")\n",
    "      except:\n",
    "        print(\"creating model...\")\n",
    "        model = create_model(df_train)\n",
    "        print(\"model created\")\n",
    "        model.save('jobs_prediction')\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        null_df = df[[f'job_{i}_category', f'job_{i}']].loc[df[f'job_{i}_category'].isnull()]\n",
    "        null_df[f'BoW_{i}'] = null_df[f'job_{i}'].apply(preprocess_text)\n",
    "        null_df[f'job_{i}_category'] = pred_jobs(model, null_df, i)\n",
    "        df.loc[df[f'job_{i}'].isin(null_df[f'job_{i}']), [f'job_{i}_category']] = null_df[\n",
    "            [f'job_{i}_category', f'job_{i}']]\n",
    "\n",
    "        df[f'job_{i}_flag'] = df.apply \\\n",
    "            (lambda row: dict_change[str(row[f'job_{i}_category']).split('.')[0]] if row[f'job_{i}'] != 'sin experiencia' else None,\n",
    "             axis=1)\n",
    "        print(f\"jobs {i} predicted\")\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', axis=1)\n",
    "    print(df.shape)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cT63CoMoNYmX"
   },
   "source": [
    "## main transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rl-SOZO6JUaK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "def find_clave(puesto, palabras_clave):\n",
    "    if puesto == 'sin experiencia':\n",
    "        return puesto\n",
    "    else:\n",
    "        for palabra in palabras_clave:\n",
    "            puesto = puesto.lower().replace('ó', 'o').replace('é', 'e').replace('í', 'i').replace('á', 'a')\n",
    "            if palabra.lower() in puesto:\n",
    "                return palabra\n",
    "\n",
    "\n",
    "def find_category(clave, categorias):\n",
    "    if clave:\n",
    "        if clave == 'sin experiencia':\n",
    "            cat_out = 0\n",
    "        else:\n",
    "            cat_out = categorias[clave]\n",
    "    else:\n",
    "        cat_out = None\n",
    "    return cat_out\n",
    "\n",
    "\n",
    "def find_job(experiencia, i):\n",
    "    try:\n",
    "        job = experiencia[i]['title'].lower()\n",
    "    except:\n",
    "        job = 'sin experiencia'\n",
    "    return job\n",
    "\n",
    "\n",
    "def find_duration(experiencia, i):\n",
    "    try:\n",
    "        year = experiencia[i]['starts_at']['year']\n",
    "        if not experiencia[i]['starts_at']['month']:\n",
    "            month = 0\n",
    "        else:\n",
    "            month = experiencia[i]['starts_at']['month']\n",
    "        if not experiencia[i]['ends_at']:\n",
    "            year_fin = time.gmtime().tm_year\n",
    "            month_fin = time.gmtime().tm_mon\n",
    "        else:\n",
    "            year_fin = experiencia[i]['ends_at']['year']\n",
    "            if not experiencia[i]['ends_at']['month']:\n",
    "                month_fin = 0\n",
    "            else:\n",
    "                month_fin = experiencia[i]['ends_at']['month']\n",
    "        duration = (year_fin - year) * 12 + (month_fin - month)\n",
    "\n",
    "    except:\n",
    "        duration = 0\n",
    "\n",
    "    return duration\n",
    "\n",
    "\n",
    "def clasif_lang(len_dict, lista_leng, cat_dict):\n",
    "    lista_clasif = []\n",
    "    if lista_leng and type(lista_leng) is not float:\n",
    "        try:\n",
    "            for i in lista_leng:\n",
    "                i = i.lower().split()[0].split(\"-\")[0].replace('é', 'e').replace('á', 'a').replace('ñ', 'Ã±').replace('ç',\n",
    "                                                                                                                      'Ã§') \\\n",
    "                    .replace(\"ê\", \"e\").replace(\"è\", \"e\").replace(\"à\", \"a\").replace(\",\", \"\").replace(\"ö\", \"o\")\n",
    "                if i in len_dict:\n",
    "                    lista_clasif.append(cat_dict[len_dict[i]])\n",
    "        except:\n",
    "            print(lista_leng)\n",
    "            raise\n",
    "    return lista_clasif\n",
    "\n",
    "\n",
    "def certis(certs):\n",
    "    resp = []\n",
    "    for i in certs:\n",
    "        if i:\n",
    "            resp.append(i['name'])\n",
    "    if resp:\n",
    "        return resp\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_string(string):\n",
    "    string = string.lower()\n",
    "    string = string.replace('ó', 'o').replace('é', 'e').replace('í', 'i').replace('á', 'a').replace('ú', 'u')\n",
    "    return string\n",
    "\n",
    "\n",
    "def data_transformation(df):\n",
    "    palabras_clave_raw = pd.read_excel('drive/MyDrive/databases/palabras_clave_jobs.xlsx', engine='openpyxl').drop('Unnamed: 1',\n",
    "                                                                                                         axis=1)\n",
    "    palabras_clave = palabras_clave_raw.palabras_clave.tolist()\n",
    "    with open('drive/MyDrive/databases/job_cat.json') as f:\n",
    "        cat_palabras = json.load(f)\n",
    "    with open('drive/MyDrive/databases/idiomas.json', encoding='latin-1') as f:\n",
    "        idiomas_dict = json.load(f)\n",
    "        # print(idiomas_dict)\n",
    "    df_comp = company_data()\n",
    "    df_cleaned = df[['_id', 'occupation', 'experiences', 'education', 'languages', 'accomplishment_courses',\n",
    "                     'certifications', 'headline', 'country', 'city', 'state']]\n",
    "\n",
    "    l = []\n",
    "    d = {}\n",
    "    l.extend(range(1, 31))\n",
    "    a = to_categorical(l, num_classes=31)\n",
    "    # para categorizar los idiomas\n",
    "    for k in range(1, 31):\n",
    "        d[k] = a[k - 1]\n",
    "\n",
    "    df_cleaned['languages'] = df_cleaned.apply(lambda row: clasif_lang(idiomas_dict, row.languages, d), axis=1)\n",
    "    df_idiomas = pd.DataFrame(df_cleaned.languages.values.tolist()).add_prefix('language_')\n",
    "    df_cleaned = pd.concat([df_cleaned, df_idiomas], axis=1)\n",
    "\n",
    "    # Limpieza y clasificación de puestos de trabajo\n",
    "    l = []\n",
    "    d_i = {}\n",
    "    l.extend(range(0, 11))\n",
    "    a = to_categorical(l, num_classes=11)\n",
    "    # para categorizar los tamaños de empresa\n",
    "    for j in range(0, 11):\n",
    "        d_i[j] = a[j]\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        df_cleaned[f'job_{i}'] = df_cleaned.apply(lambda row: find_job(row.experiences, i - 1), axis=1)\n",
    "        df_cleaned[f'job_{i}_flag'] = df_cleaned.apply(lambda row: find_clave(row[f'job_{i}'], palabras_clave), axis=1)\n",
    "        df_cleaned[f'job_{i}_category'] = df_cleaned.apply(\n",
    "            lambda row: find_category(row[f'job_{i}_flag'], cat_palabras), axis=1)\n",
    "        df_cleaned[f'job_{i}_duration'] = df_cleaned.apply(lambda row: find_duration(row.experiences, i - 1), axis=1)\n",
    "        df_cleaned = company_enrichment(df_cleaned, df_comp, i, d_i)\n",
    "\n",
    "    print(df_cleaned.shape)\n",
    "    # limpieza educacion\n",
    "    df_cleaned['estudios'] = df_cleaned.apply(\n",
    "        lambda row: row.education[0]['degree_name'] if row.education and type(row.education) == list else None,\n",
    "        axis=1)\n",
    "    df_cleaned['universidad'] = df_cleaned.apply(\n",
    "        lambda row: row.education[0]['school'] if row.education and type(row.education) == list else None,\n",
    "        axis=1)\n",
    "    df_cleaned['certifications'] = df_cleaned.apply(\n",
    "        lambda row: certis(row.certifications) if type(row.certifications) == list else None, axis=1)\n",
    "    df_cleaned['accomplishment_courses'] = df_cleaned.apply(\n",
    "        lambda row: certis(row.accomplishment_courses) if type(row.certifications) == list else None, axis=1)\n",
    "    df_cleaned = df_cleaned.drop(['experiences', \"education\"], axis=1)\n",
    "    df_edited = df_cleaned.loc[df_cleaned['job_1'] != 'Sin experiencia']\n",
    "    # print(df_edited[['job_1', 'job_1_flag', 'job_1_category']])\n",
    "    incogs = df_cleaned.loc[df_cleaned['job_1_category'].isnull()]\n",
    "    # print(incogs[['job_1']].head(10))\n",
    "\n",
    "    print(f\"\\nNº de puestos no clasificados: {incogs.shape[0]} puestos\")\n",
    "    print(f\"porcentaje del total: {round(100 * (incogs.shape[0] / df_edited.shape[0]), 1)}%\")\n",
    "\n",
    "    df_edited_2 = jobs_prediction(df_edited)\n",
    "    print(\"jobs predicted, finishing categorization...\")\n",
    "    lista_trabajos = df_edited_2['job_1_category'].unique().tolist()\n",
    "    dict_cat = {}\n",
    "    l = []\n",
    "    l.extend(range(0, len(lista_trabajos) + 1))\n",
    "    i = 0\n",
    "    a = to_categorical(l, num_classes=len(l))\n",
    "    for trab in lista_trabajos:\n",
    "        dict_cat[trab] = a[i]\n",
    "        i += 1\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        df_cleaned[f'job_{i}_class'] = df_cleaned.apply(\n",
    "            lambda row: dict_cat[row[f'job_{i}_category']] if not row[f'job_{i}_category'] or type(\n",
    "                row[f'job_{i}_category'])   != float else None, axis=1)\n",
    "    df_edited_2 = df_edited_2.drop_duplicates(subset=['_id'])\n",
    "    return df_edited_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yMbj11xJ7gX"
   },
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJwRfTSIJ_UP",
    "outputId": "dbcd4a6e-5876-402e-f6bf-145a8778db9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12369, 98)\n",
      "\n",
      "Nº de puestos no clasificados: 3794 puestos\n",
      "porcentaje del total: 30.7%\n",
      "\n",
      " Starting jobs prediction...\n",
      "creating model...\n",
      "init: 9339\n",
      "model created\n",
      "INFO:tensorflow:Assets written to: jobs_prediction/assets\n",
      "jobs 1 predicted\n",
      "jobs 2 predicted\n",
      "jobs 3 predicted\n",
      "jobs 4 predicted\n",
      "jobs 5 predicted\n",
      "jobs 6 predicted\n",
      "(12369, 98)\n",
      "jobs predicted, finishing categorization...\n",
      "Proceso finalizado, tenga un buen día\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# pip3 install openpyxl (para exportar a excel)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'    # quitar los logs de tensor\n",
    "    # create_jobs_table()\n",
    "    df1 = data_extract()\n",
    "    df2 = data_transformation(df1)\n",
    "\n",
    "    #df2.to_excel(\"data_cleaned2.xlsx\")\n",
    "    df2.to_csv(\"data_cleaned2.csv\")\n",
    "    print('Proceso finalizado, tenga un buen día')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46ZAnNCNOnDt"
   },
   "source": [
    "# Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA8JPSFwOiBu"
   },
   "outputs": [],
   "source": [
    "#records = json.loads(df2.T.to_json()).values()\n",
    "#db.myCollection.insert_many(records)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "837PCQ4WJs8O",
    "cT63CoMoNYmX"
   ],
   "name": "ETL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
